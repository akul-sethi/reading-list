This repo contains a collection of research papers I have read and enjoyed, and my notes on them. They mostly involve computer science, but can be about anything. 

| Paper | Description | Have Read? |
|-------| ------------| -----------|
| [ART: Automatic multi-step reasoning and tool-use for large language models](https://arxiv.org/abs/2303.09014) | A tool based method for code editing. |❌|
| [CodeEditor: Learning to Edit Source Code with Pre-trained Models](https://arxiv.org/pdf/2210.17040.pdf) | Another code editing paper. |❌|
|[SOFTWARE ASPECTS OF STRATEGIC DEFENSE SYSTEMS](https://dl.acm.org/doi/pdf/10.1145/214956.214961 ) | The state of software engineering and predictions on its future written by a disgruntled computer scientist in 1985. Amazing read. |✅|
|[Diff Models – A New Way to Edit Code](https://carper.ai/diff-models-a-new-way-to-edit-code/) | Code editing using diffs rather than generation from scratch. | ❌|
| [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084)| Essential sentence embedding read. |❌|
|[Source Code Data Augmentation for Deep Learning: A Survey](https://arxiv.org/pdf/2305.19915.pdf) | A survey on data augmentation for code generation. |✅|
| [CodeBLEU: a Method for Automatic Evaluation of Code Synthesis](https://arxiv.org/abs/2009.10297 ) | A twist on the popular lexical similarity metric: BLEU. |❌|
|[MIXCODE: Enhancing Code Classification by Mixup-Based Data Augmentation](https://arxiv.org/pdf/2210.03003.pdf) | Creates new training samples by combining two other embeddings. |❌|
|[On the expressive power of programming languages](https://www.sciencedirect.com/science/article/pii/016764239190036W) | On expressivity of programming languages. Tough yet interesting read, |✅|
|[SWE-bench: Can Language Models Resolve Real-World GitHub Issues?](https://arxiv.org/abs/2310.06770) | Benchmark paper for code editing. Sourced from git pull requests so it is not great for evaluating models. |❌|
|[Evaluating Large Language Models Trained on Code](https://arxiv.org/abs/2107.03374) | Introduces pass@k and HumanEval; essential in evaluating CodeLLMs. Additionally, it introduces Codex, an LLM trained on code. |✅|
|[InCoder: A Generative Model for Code Infilling and Synthesis](https://arxiv.org/abs/2204.05999) | Introduces infilling, a multi-location variation on fill-in-the-middle (FIM) |✅|
|[MultiPL-E: A Scalable and Polyglot Approach to Benchmarking Neural Code Generation](https://ieeexplore.ieee.org/document/10103177) | Generating benchmarks in alternative languages. Useful for benchmarking CodeLLMs against low resource langauges. |✅|
|[Knowledge Transfer from High-Resource to Low-Resource Programming Languages for Code LLMs](https://arxiv.org/abs/2308.09895) | Using High-Resource language data for training CodeLLMs to operate on Low-Resource languages by a unique transpilation process. |✅|
|[StarCoder: may the source be with you!](https://arxiv.org/abs/2305.06161) | Classic CodeLLM paper by BigCode. The goal is to create a CodeLLM which is publically available.|✅|
|[Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) | Original GPT paper. Not much needs to be said. |✅|
|[Attention Is All You Need](https://arxiv.org/abs/1706.03762) | The paper which started it all. Introduced the Transformer Architecture and the rest is history. |❌|
|[Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/pdf/1508.04025.pdf)|Explains attention quite well in the context of recurrent neurel networks.|✅|
|[Understanding and Coding the Self-Attention Mechanism of Large Language Models From Scratch](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html)|Good resource for coding and understanding attention.|
|[The Building Blocks of Interprtability](https://distill.pub/2018/building-blocks/)| Explains the fundementals of interp. Very interesting read.|✅|
|[Zoom In: An Introduction to Circuits](https://distill.pub/2020/circuits/zoom-in/)|Brief explanation on circuits.|✅|
|[A Path Towards Autonomous Machine Intelligence](https://openreview.net/pdf?id=BZ5a1r-kVsf)|Yann Lecun's vision on AI in the future. Introduces Joint-Embedding Predictive Architecture. |✅|

